{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "**Python Topics** - Parallel Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "I have a python function which I'm trying to speed up. can you help me with parallelizing it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "A couple of immediate options which come to mind are:\n",
    "- Using the `multiprocessing` module\n",
    "- Using the `threading` module\n",
    "\n",
    "Mainly the choice between them depends on the type of operations the function is doing. If you're reading files or sending network requests... Threading is the way to go. If you're doing CPU intensive operations like processesing a lot of data, then multiprocessing should be your go to.\n",
    "\n",
    "#### Multiprocessing\n",
    "```python\n",
    "import multiprocessing\n",
    "\n",
    "def my_function():\n",
    "    # do something\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    processes = []\n",
    "    for _ in range(10):\n",
    "        p = multiprocessing.Process(target=my_function)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "```\n",
    "\n",
    "#### Threading\n",
    "```python\n",
    "import threading\n",
    "\n",
    "def my_function():\n",
    "    # do something\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threads = []\n",
    "    for _ in range(10):\n",
    "        t = threading.Thread(target=my_function)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "\n",
    "```\n",
    "That just shows the concept... but in your case, we'll be parallelizing the insides of your funciton. \n",
    "If you send give me the function I could just transform it for you :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "I can't share it exactly, here's similar function\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def heavy_computation(data):\n",
    "    def compute(item, supplement):\n",
    "        result = math.sqrt(item) * math.log(item) + math.exp(item % 10) + supplement\n",
    "        return result\n",
    "\n",
    "    # Apply heavy computation to each item in the list\n",
    "    processed_data = [compute(item) for item in data]\n",
    "    return processed_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Well okay, I can see that most of the time is goint to be spent in the compute function. I think the best way to parallelize this is to use the `multiprocessing` module since this is a CPU intensive operation. \n",
    "Pool.starmap is my favorite way to do this because it keeps your code clean and handles the process management under the hood.\n",
    "\n",
    "```python\n",
    "import math\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "def compute(item):\n",
    "    result = math.sqrt(item) * math.log(item) + math.exp(item % 10) + item**2\n",
    "    return result\n",
    "\n",
    "\n",
    "def parallelized_heavy_computation(data):\n",
    "    args = [(item,) for item in data]\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "        processed_data = pool.starmap(compute, args)\n",
    "\n",
    "    return processed_data\n",
    "```\n",
    "\n",
    "Here are the changes I had to make\n",
    "- Move the compute function to the global scope, functions get pickled as they are passed between processes & local functions are not picklable.\n",
    "- Replaced the computation of the supplement variable to be inside the compute function. Since this is the only place it's being used... this way it can be parallelized.\n",
    "- Leveraged `Pool.starmap()` in order to distribute the computation across all cores on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "What's the difference between `multiprocessing` and `threading`? Why is multiprocessing better for CPU intensive operations and not threading? Why is threading better for IO operations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "`multiporcessing` creates new os-level processes, these have the benefit of getting their own memory space and more importantly, different processes may use different CPU cores. `threading` on the other hand, creates threads which are all part of the same process, they share the same memory space and can only use one CPU core, they just allow the code to avoid `waiting` and do other things while waiting for IO operations to complete.\n",
    "\n",
    "This causes `threading` to have lower overhead than `multiprocessing`` since it doesn't have to create new processes, but it also means that it can't take advantage of multiple CPU cores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
