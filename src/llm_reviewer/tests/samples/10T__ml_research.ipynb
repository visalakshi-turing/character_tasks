{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "**Python Topics** - Machine Learning using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "Do you know the iris dataset for ml? I wanted to explore scikit learn algorithms on it. Give me the code to solve it using some algo from scikit learn. I'm in an empty folder right now. linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Yes, I know the iris dataset that is commonly used as a toy dataset to showcase the work of machine learning classification algorithms.\n",
    "\n",
    "scikit-learn has multiple algorithms that are capable of \"solving\" it, let's start with a simple logistic regression.\n",
    "\n",
    "But first, to start we will need to set up an environment with scikit-learn python package installed.\n",
    "\n",
    "Assuming that you are in a project root directory, let's set up a virtual environment for this task.\n",
    "\n",
    "Run the following in the command line:\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "\n",
    "This will create a folder named venv with the virtual environment for you.\n",
    "\n",
    "Next, you will need to activate it and install the scikit-learn package using pip\n",
    "\n",
    "```\n",
    "source venv/bin/activate && pip install scikit-learn\n",
    "```\n",
    "\n",
    "Now with all this out of the way, here's the code to actually train an algorithm to solve this dataset. \n",
    "\n",
    "iris_tests.py\n",
    "```\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Classification accuracy using logistic regression:\", accuracy)\n",
    "```\n",
    "\n",
    "All you have to do is `python iris_tests.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "I got 1.0. That was pretty good. Why 42 though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Just keeping in with the tradition - 42 is a humorous number from the book The Hitchhiker's Guide to the Galaxy by Douglas Adams. In the book, the number was the answer to \"the Ultimate Question of Life, the Universe, and Everything\" provided by a supercomputer when it was asked to answer the ultimate question. So, it's an inside joke for the programmers to use it in various places, often as a seed for a random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "lol. Alright.\n",
    "\n",
    "Back to the scikit, now that I know it works, show me multiple algos to solve it at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "The package has quite a few ml algorithms that would work on this dataset. Let's use some of them:\n",
    "\n",
    "Create a new file called `many_classifiers_on_iris.py`\n",
    "\n",
    "The code:\n",
    "\n",
    "```\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# First we will gather all the algorithms into a single array as scikit provides a universal API to work with the algorithms - very convenient\n",
    "algorithms = []\n",
    "\n",
    "# Initialize Logistic Regression as the last time\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "algorithms.append({'algo': logreg, 'scale': True})\n",
    "\n",
    "# Initialize the K-Nearest Neighbors classifier with n_neighbors=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "algorithms.append({'algo': knn, 'scale': True})\n",
    "\n",
    "# Create a SVM Classifier\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "algorithms.append({'algo': svm_classifier, 'scale': True})\n",
    "\n",
    "# Creating a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "algorithms.append({'algo': clf, 'scale': False})\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "algorithms.append({'algo': rf_classifier, 'scale': False})\n",
    "\n",
    "# Initialize GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "algorithms.append({'algo': gb_clf, 'scale': True})\n",
    "\n",
    "# Creating a Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "algorithms.append({'algo': gnb, 'scale': False})\n",
    "\n",
    "# Now we can loop over the algorithms list and apply each one\n",
    "for algorithm in algorithms:\n",
    "    current_x = X_train\n",
    "    current_x_test = X_test\n",
    "    \n",
    "    if algorithm['scale']:\n",
    "        current_x = scaled_X_train\n",
    "        current_x_test = scaled_X_test\n",
    "    \n",
    "    algorithm['algo'].fit(current_x, y_train)\n",
    "    y_pred = algorithm['algo'].predict(current_x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Classification accuracy using\", type(algorithm['algo']).__name__, \":\", accuracy)\n",
    "\n",
    "```\n",
    "\n",
    "You can now run `python many_classifiers_on_iris.py` and see the accuracy printed for each one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "Got 1.0 for almost all of them. Do you know why SVM is called SVC not SVMC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "In SVM or Support Vector Machine, \"machine\" just means it's a system that learns from data to make predictions. When SVM is used for sorting data into categories, people call it \"Support Vector Classification\" or SVC in scikit-learn. This name change simply points out that the SVM approach is specifically being used for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "I'm wondering about about a more difficult dataset, do you have any in mind that would worsen the results? datasets from scikit datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "To find a more challenging dataset for the classifiers we have, we can move them into a separate module and call it like a function for a given dataset. After testing it on a few datasets, we can find a more challenging dataset for the algorithms we have given the same hyperparameters.\n",
    "\n",
    "Create a file classifiers_set.py\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_classifiers(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize features by removing the mean and scaling to unit variance for the algorithms that benefit from it\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X_train = scaler.fit_transform(X_train)\n",
    "    scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Gather all the algorithms into a single array\n",
    "    algorithms = []\n",
    "\n",
    "    # Initialize Logistic Regression as the last time\n",
    "    logreg = LogisticRegression(max_iter=200)\n",
    "    algorithms.append({'algo': logreg, 'scale': True})\n",
    "\n",
    "    # Initialize the K-Nearest Neighbors classifier with n_neighbors=3\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    algorithms.append({'algo': knn, 'scale': True})\n",
    "\n",
    "    # Create a SVM Classifier\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "    algorithms.append({'algo': svm_classifier, 'scale': True})\n",
    "\n",
    "    # Creating a Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier()\n",
    "    algorithms.append({'algo': clf, 'scale': False})\n",
    "\n",
    "    # Initialize RandomForestClassifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "    algorithms.append({'algo': rf_classifier, 'scale': False})\n",
    "\n",
    "    # Initialize GradientBoostingClassifier\n",
    "    gb_clf = GradientBoostingClassifier()\n",
    "    algorithms.append({'algo': gb_clf, 'scale': True})\n",
    "\n",
    "    # Creating a Gaussian Naive Bayes model\n",
    "    gnb = GaussianNB()\n",
    "    algorithms.append({'algo': gnb, 'scale': False})\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Loop over the algorithms list and apply each one\n",
    "    for algorithm in algorithms:\n",
    "        current_x = X_train\n",
    "        current_x_test = X_test\n",
    "\n",
    "        if algorithm['scale']:\n",
    "            current_x = scaled_X_train\n",
    "            current_x_test = scaled_X_test\n",
    "\n",
    "        algorithm['algo'].fit(current_x, y_train)\n",
    "        y_pred = algorithm['algo'].predict(current_x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results.append({'classifier': type(algorithm['algo']).__name__, 'accuracy': accuracy})\n",
    "\n",
    "    return results\n",
    "\n",
    "```\n",
    "\n",
    "And to test multiple datasets let's create a file `test_multiple_datasets.py`\n",
    "\n",
    "```\n",
    "from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer\n",
    "from classifiers_set import run_classifiers\n",
    "\n",
    "datasets = [load_iris, load_digits, load_wine, load_breast_cancer]\n",
    "\n",
    "for dataset in datasets:\n",
    "    X, y = dataset(return_X_y=True)\n",
    "    results = run_classifiers(X, y)\n",
    "    print(f\"Results for {dataset.__name__}:\")\n",
    "    for result in results:\n",
    "        print(f\"Classifier: {result['classifier']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To check the results: `python test_multiple_datasets.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "Huh. These basic classifiers tore through all the datasets you've tried. I think what would be interesting to do now is to get back to our iris dataset and see how the classifiers handle noise in the dataset. Add 5% disturbance for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Interesting result and a great idea on how to explore these classifiers more!\n",
    "To add noise to the training set of a dataset and run through the classifiers we have you will need to modify the run classifiers function:\n",
    "\n",
    "Replace\n",
    "```\n",
    "def run_classifiers(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "with\n",
    "```\n",
    "def run_classifiers(X, y, X_test=None, y_test=None):\n",
    "    if X_test is None:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        X_train, y_train = X, y\n",
    "```\n",
    "With this you will be able to provide run_classifiers with the modified training data and unmodified test data.\n",
    "\n",
    "Now create a file `test_with_noise.py`\n",
    "\n",
    "```\n",
    "from sklearn.datasets import load_iris\n",
    "from classifiers_set import run_classifiers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def add_noise(X_train, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    Function to add uniform random noise to the features in %\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-noise_level, noise_level, X_train.shape)\n",
    "    return X_train + X_train*noise\n",
    "\n",
    "def run_experiment(noise_level):\n",
    "    \"\"\"\n",
    "    Function to run the experiment with the classifiers.\n",
    "    \"\"\"\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_noisy = add_noise(X_train, noise_level)\n",
    "    results = run_classifiers(X_train_noisy, y_train, X_test, y_test)\n",
    "    print(f\"Results for {load_iris.__name__}:\")\n",
    "    for result in results:\n",
    "        print(f\"Classifier: {result['classifier']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(0.05)\n",
    "```\n",
    "\n",
    "Run it with `python test_with_noise.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "Result is still too good. Do a grid search over the noise level to find a tipping point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Wonderful idea. Here's the code to look for a noise level that degrades at least one classifier to less than 50% accuracy.\n",
    "I've split the noise level into 5% increments for this search.\n",
    "\n",
    "Add ```return results``` to `test_with_noise.py` file at the end of run_experiment function.\n",
    "\n",
    "Then create a file `noise_level_search_on_iris.py`\n",
    "\n",
    "```\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from test_with_noise import run_experiment\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def grid_search_noise_levels():\n",
    "    \"\"\"\n",
    "    Function to perform a grid search over the noise levels.\n",
    "    \"\"\"\n",
    "    noise_levels = np.linspace(0, 1, 21)\n",
    "    for noise_level in noise_levels:\n",
    "        print('-'*60)\n",
    "        print(f\"Results for noise level {noise_level}:\")\n",
    "        results = run_experiment(noise_level)\n",
    "        if any(result['accuracy'] < 0.5 for result in results):\n",
    "            print('First <0.5 accuracy result')\n",
    "            return\n",
    "\n",
    "grid_search_noise_levels()\n",
    "\n",
    "```\n",
    "\n",
    "Run it with: `python noise_level_search_on_iris.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "This is too much information on the screen for me. I want you to show me how to plot this data as a line for each of the classifiers depending on the noise level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "To plot the results, you will need to install another package - matplotlib.\n",
    "Run `pip install matplotlib`.\n",
    "\n",
    "Next, replace the `noise_level_search_on_iris.py` with the following code:\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from test_with_noise import run_experiment\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def grid_search_noise_levels():\n",
    "    \"\"\"\n",
    "    Function to perform a grid search over the noise levels.\n",
    "    \"\"\"\n",
    "    noise_levels = np.linspace(0, 1, 21)\n",
    "    all_results = []\n",
    "    for noise_level in noise_levels:\n",
    "        print('-'*60)\n",
    "        print(f\"Results for noise level {noise_level}:\")\n",
    "        results = run_experiment(noise_level)\n",
    "        all_results.append((noise_level, results))\n",
    "        if any(result['accuracy'] < 0.5 for result in results):\n",
    "            return all_results\n",
    "    return all_results\n",
    "\n",
    "def plot_results(all_results):\n",
    "    \"\"\"\n",
    "    Function to plot the results.\n",
    "    \"\"\"\n",
    "    classifier_names = [result['classifier'] for result in all_results[0][1]]\n",
    "    noise_levels = [result[0] for result in all_results]\n",
    "    for i, classifier_name in enumerate(classifier_names):\n",
    "        accuracies = [result[1][i]['accuracy'] for result in all_results]\n",
    "        plt.plot(noise_levels, accuracies, label=classifier_name)\n",
    "    plt.xlabel('Noise level')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('results.png')\n",
    "\n",
    "all_results = grid_search_noise_levels()\n",
    "plot_results(all_results)\n",
    "```\n",
    "\n",
    "After running `python noise_level_search_on_iris.py` you should see `results.png` file in the folder with the chart we created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "I've generated the image. Give me a cluster bar chart for this thing.\n",
    "But reduce number of noise points to one per 30%. Make the bars thin as we have lots of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Sure thing. Add this function to the `noise_level_search_on_iris.py` after `plot_results(all_results)`\n",
    "\n",
    "```\n",
    "def plot_results_cluster_bar(all_results):\n",
    "    \"\"\"\n",
    "    Function to plot the results.\n",
    "    \"\"\"\n",
    "    classifier_names = [result['classifier'] for result in all_results[0][1]]\n",
    "    print(all_results)\n",
    "    noise_levels = [result[0] for result in all_results if result[0] % 0.3 < 0.001]\n",
    "    print(noise_levels)\n",
    "    width = 0.1 / len(classifier_names)  # Adjust bar width based on number of classifiers\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, classifier_name in enumerate(classifier_names):\n",
    "        accuracies = [result[1][i]['accuracy'] for result in all_results if result[0] % 0.3  < 0.001]\n",
    "        ax.bar([noise + i * width for noise in noise_levels], accuracies, width, label=classifier_name)\n",
    "    ax.set_xlabel('Noise level')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend()\n",
    "    plt.savefig('results_cluster_bar.png')\n",
    "```\n",
    "\n",
    "and then add this line at the end of the file to create the new chart - `plot_results_cluster_bar(all_results)`.\n",
    "\n",
    "The new image will be named `results_cluster_bar.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Human**\n",
    "\n",
    "Thanks! What was the point of doing < 0.001 in the code above instead of == 0 though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "I've used < 0.001 comparisons instead of == 0 to avoid problems with the float point numbers precision. As we used `np.linspace(0, 1, 21)` to generate the noise levels, we might not get precise values for 0.3 0.6 0.9 even though we've separated [0, 1] into 21 points due to how float numbers are stored in the computer. So to avoid a situation, where a very small difference prevents numbers from being seen as equal, I've added a threshold to ignore these small differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
